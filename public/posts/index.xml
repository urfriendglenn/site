<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Terminal</title>
    <link>/posts/</link>
    <description>Recent content in Posts on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 24 Sep 2024 21:51:21 -0500</lastBuildDate><atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Certified Kubernetes Administrator: Scheduling</title>
      <link>/posts/cka_3/</link>
      <pubDate>Tue, 24 Sep 2024 21:51:21 -0500</pubDate>
      
      <guid>/posts/cka_3/</guid>
      <description>Manual Scheduling How Scheduling Works Every pod has a field called nodeName that is blank by default
The scheduler looks through all pods and searches for pods without this field set
Those pods are candidates for being scheduled
It then assigns the pod to the node based on the scheduling algorithm
It then assigns the node by setting the value to the nodeName field
Nodes can be assigned statically in the definition file as well</description>
      <content>&lt;h1 id=&#34;manual-scheduling&#34;&gt;Manual Scheduling&lt;/h1&gt;
&lt;h2 id=&#34;how-scheduling-works&#34;&gt;How Scheduling Works&lt;/h2&gt;
&lt;p&gt;Every pod has a field called &lt;code&gt;nodeName&lt;/code&gt; that is blank by default&lt;br&gt;
The scheduler looks through all pods and searches for pods without this field set&lt;br&gt;
Those pods are candidates for being scheduled&lt;br&gt;
It then assigns the pod to the node based on the scheduling algorithm&lt;br&gt;
It then assigns the node by setting the value to the &lt;code&gt;nodeName&lt;/code&gt; field&lt;br&gt;
Nodes can be assigned statically in the definition file as well&lt;br&gt;
Kubernetes will not allow the value change of the &lt;code&gt;nodeName&lt;/code&gt; field to an existing pod, so a binding object must be created then posted to the node&amp;rsquo;s binding API:&lt;br&gt;
&lt;code&gt;pod-bind-definition.yaml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Binding
metadata:
	name: nginx
target:
	apiVersion: v1
	kind: Node
	name:
&lt;/code&gt;&lt;/pre&gt;</content>
    </item>
    
    <item>
      <title>Certified Kubernetes Administrator: Core Concepts</title>
      <link>/posts/cka_2/</link>
      <pubDate>Mon, 23 Sep 2024 19:44:48 -0500</pubDate>
      
      <guid>/posts/cka_2/</guid>
      <description>Cluster Architecture Control Plane Master Node: Manage, Plan, Schedule, and Monitor Nodes
ETCD Cluster: Highly available key/value database
kube-scheduler: Identifies the right node to place a container on
Node-Controller: Manages nodes
Replication-Controller: Ensures desired number of containers are running in a replication group
kube-apiserver: Orchestrates all operations within the cluster and exposes the API\
Workers Worker Nodes: Host applications as Containers
A container runtime engine, like Docker, must be installed and available on all nodes</description>
      <content>&lt;h1 id=&#34;cluster-architecture&#34;&gt;Cluster Architecture&lt;/h1&gt;
&lt;h2 id=&#34;control-plane&#34;&gt;Control Plane&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Master Node&lt;/strong&gt;: Manage, Plan, Schedule, and Monitor Nodes&lt;br&gt;
&lt;strong&gt;ETCD Cluster&lt;/strong&gt;: Highly available key/value database&lt;br&gt;
&lt;strong&gt;kube-scheduler&lt;/strong&gt;: Identifies the right node to place a container on&lt;br&gt;
&lt;strong&gt;Node-Controller&lt;/strong&gt;: Manages nodes&lt;br&gt;
&lt;strong&gt;Replication-Controller&lt;/strong&gt;: Ensures desired number of containers are running in a replication group&lt;br&gt;
&lt;strong&gt;kube-apiserver&lt;/strong&gt;: Orchestrates all operations within the cluster and exposes the API\&lt;/p&gt;
&lt;h2 id=&#34;workers&#34;&gt;Workers&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Worker Nodes&lt;/strong&gt;: Host applications as Containers&lt;br&gt;
A container runtime engine, like Docker, must be installed and available on all nodes&lt;br&gt;
&lt;strong&gt;kubelet&lt;/strong&gt;: Agent that runs on each node in a cluster and listens for instructions from the API as well as monitors status of all the other node
&lt;strong&gt;kube-proxy&lt;/strong&gt;: Service ensures that the rules are in place on the worker nodes so the containers can communicate between each othe&lt;/p&gt;
&lt;h1 id=&#34;docker-vs-containerd&#34;&gt;Docker vs. Containerd&lt;/h1&gt;
&lt;p&gt;Docker support was removed from Kubernetes, but Docker images still work because they adhere to imagespec&lt;br&gt;
&lt;strong&gt;containerd&lt;/strong&gt;: Container runtime used by Docker that can be ran by itself&lt;br&gt;
&lt;code&gt;ctr&lt;/code&gt;: Debugging tool bundled with containerd&lt;br&gt;
&lt;code&gt;nerdctl&lt;/code&gt;: Provides a Docker-like for containerd&lt;br&gt;
      Supports docker compose&lt;br&gt;
      Supports newest features in containerd&lt;br&gt;
&lt;code&gt;crictl&lt;/code&gt;: Provides a CLI for Container Runtime Interface (CRI) compatible container runtimes&lt;br&gt;
      Installed separately&lt;br&gt;
      Used to inspect and debug container runtimes&lt;br&gt;
      Works across different runtimes&lt;/p&gt;
&lt;h1 id=&#34;etcd-for-beginners&#34;&gt;ETCD for Beginners&lt;/h1&gt;
&lt;p&gt;ETCD is a distributed reliable key-value store&lt;br&gt;
&lt;strong&gt;Key-Value Store&lt;/strong&gt;: Stores information as documents and all information on an item is contained in the corresponding document&lt;br&gt;
To install ETCD, download and extract the binary then run the service&lt;br&gt;
It will run on port 2379 and clients can be attached&lt;br&gt;
&lt;code&gt;etcdctl&lt;/code&gt;: Command client&lt;br&gt;
      &lt;code&gt;etcdctl put key1 value1&lt;/code&gt;: Sets a key&lt;br&gt;
      &lt;code&gt;ectdctl get key1&lt;/code&gt;: Gets a key value&lt;/p&gt;
&lt;h1 id=&#34;etcd-in-kubernetes&#34;&gt;ETCD in Kubernetes&lt;/h1&gt;
&lt;p&gt;Stores information on the Nodes, PODs, Configs, Secrets, etc.&lt;br&gt;
All changes are updated in the ETCD server, and is not considered completed until changed\&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;manual&#34;&gt;Manual&lt;/h3&gt;
&lt;p&gt;The binary is downloaded, then manually set as a service by executing the &lt;code&gt;etcd&lt;/code&gt; command with the required options&lt;/p&gt;
&lt;h2 id=&#34;kubeadm&#34;&gt;kubeadm&lt;/h2&gt;
&lt;p&gt;ETCD is deployed as a pod in the clusted&lt;br&gt;
&lt;code&gt;kubectl exec etcd-master -n kube-system etcdctl get / --prefic -keys-only&lt;/code&gt;: Get keys stored in the ETCD pod\&lt;/p&gt;
&lt;p&gt;In a HA Environment, there are multiple ETCD masters&lt;/p&gt;
&lt;h1 id=&#34;kube-api-server&#34;&gt;Kube-API Server&lt;/h1&gt;
&lt;p&gt;When running a command, it is sent to the kube-apiserver&lt;br&gt;
POST requests can also be sent in place of commands:&lt;br&gt;
&lt;code&gt;curl -X POST /api/v1/namespaces/default/pods ...&lt;/code&gt;: Creates a Pod&lt;br&gt;
      The scheduler detects there is a pod with no node assigned, then chooses a pod and communicates to the kube-apiserver&lt;br&gt;
      The apiserver instructs the kubelet to create a pod&lt;br&gt;
      The kubelet communicates the change to the apiserver, which updates ETCD&lt;/p&gt;
&lt;h1 id=&#34;kube-controller-manager&#34;&gt;Kube Controller Manager&lt;/h1&gt;
&lt;p&gt;A controller is a process that continuously monitors the system and takes necessary actions to keep the system in a specified state&lt;br&gt;
&lt;strong&gt;Node Controller&lt;/strong&gt;: Responsible for monitoring the status of the nodes and takes action to keep applications running&lt;br&gt;
      Checks status every 5 second&lt;br&gt;
      After 40 seconds of no heartbeat, a node is marked unreachable&lt;br&gt;
      After 5 minutes, the node is evicted and pods are moved to healthy nodes&lt;br&gt;
&lt;strong&gt;Replication Controller&lt;/strong&gt;: Responsible for monitoring the status of replicasets and ensures the number of pods are available within a set&lt;br&gt;
      If a pod dies, it creates another pod&lt;br&gt;
There are many types of controllers in a Kubernetes Cluster, and are all packaged in the &lt;strong&gt;Kube-Controller-Manager&lt;/strong&gt;&lt;br&gt;
kubeadm deploys the kube-controller-manager as a pod\&lt;/p&gt;
&lt;h1 id=&#34;kube-scheduler&#34;&gt;Kube Scheduler&lt;/h1&gt;
&lt;p&gt;Responsible which pod goes on which node&lt;br&gt;
Does not actually place the pod, the kubelet does&lt;br&gt;
The scheduler looks at each pod and finds the correct node for it based on requirements (CPU, Memory, Labels, etc.)&lt;br&gt;
The scheduler ranks the nodes and assigns on a rank from 0-10&lt;br&gt;
kubeadm deploys the scheduler as a pod&lt;/p&gt;
&lt;h1 id=&#34;kubelet&#34;&gt;Kubelet&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Registers nodes&lt;/li&gt;
&lt;li&gt;Creates pods&lt;/li&gt;
&lt;li&gt;Monitors nodes and pods
The kubelet service is not installed by kubeadm and must be manually installed on each node&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;kube-proxy&#34;&gt;Kube Proxy&lt;/h1&gt;
&lt;p&gt;Every pod can reach every pod&lt;br&gt;
A pod network is an internal network each pod communicates with&lt;br&gt;
Applications are deployed as services so they can be available to the cluster&lt;br&gt;
The service has an assigned IP&lt;br&gt;
&lt;strong&gt;kube-proxy&lt;/strong&gt;: Process that runs on each that looks for new services, then creates the appropriate rules to route traffic through the pods&lt;br&gt;
kubeadm deploys kube-proxy as a pod&lt;/p&gt;
&lt;h1 id=&#34;pods&#34;&gt;Pods&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Pod&lt;/strong&gt;: A single instance of an application and is the smallest object that can be created in Kubernetes&lt;br&gt;
Containers are not added to existing pods to scale&lt;br&gt;
Pods can container multiple containers, but the containers are not of the same image&lt;br&gt;
&lt;code&gt;kubectl run nginx --image nginx&lt;/code&gt;: Deploys a docker container by creating a pod using the NGINX Docker Image&lt;br&gt;
&lt;code&gt;kubectl get pods&lt;/code&gt;: Lists Pods&lt;/p&gt;
&lt;h1 id=&#34;pods-with-yaml&#34;&gt;Pods with YAML&lt;/h1&gt;
&lt;p&gt;Kubernetes uses YAML files as inputs for creation of objects&lt;br&gt;
A Kubernetes definition file always contains the following four top-level fields:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
	name: myapp-prod
	labels:
		app: myapp
		type: front-end
spec:
	containers:
		- name: nginx-container
		- image: nginx
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt;: The version of the API the object will be created with&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt;: Type of object being created&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metadata&lt;/code&gt;: Data about the object in the form of a dictionary&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec&lt;/code&gt;: Specifications pertaining to the object
&lt;code&gt;kubectl create -f pod-definition.yml&lt;/code&gt;: Creates the object listed in the specified file
&lt;code&gt;kubectl describe pod myapp-pod&lt;/code&gt;: Lists information on the specified pod&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;replicasets&#34;&gt;ReplicaSets&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Replication Controller&lt;/strong&gt;: Runs multiple instances of a single Pod in a cluster&lt;br&gt;
A replication controller also brings up new pods when a pod fails and ensures the necessary number of pods are running&lt;br&gt;
Replication assists with load balancing and scaling by deploying multiple nodes in the cluster across nodes&lt;br&gt;
&lt;strong&gt;ReplicaSet&lt;/strong&gt;: New recommended utility to manage replication&lt;br&gt;
To create a replication controller:
&lt;code&gt;rc-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
	name: myapp-rc
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-prod
			labels:
				app: myapp
				type: front-end
	spec:
		containers:
		- name: nginx-container
	      image: nginx
	replicas: 3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;template&lt;/code&gt;: Pod template to be created by the replication controller, which will contain the contents of a Pod YAML file&lt;br&gt;
&lt;code&gt;kubectl create -f rc-definition.yml&lt;/code&gt;: Creates the replication controller&lt;br&gt;
&lt;code&gt;kubectl get replicationcontroller&lt;/code&gt;: Lists replication controller\&lt;/p&gt;
&lt;p&gt;To create a replicaset:&lt;br&gt;
&lt;code&gt;replicaset-definition.yml&lt;/code&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: myapp-replicaset
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-prod
			labels:
				app: myapp
				type: front-end
	spec:
		containers:
		- name: nginx-container
	      image: nginx
	replicas: 3
	selector:
		matchLabels:
			type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;selector&lt;/code&gt;: Identifies applicable pods, as ReplicaSets can manage pods that were not created by it. It is not a required field&lt;br&gt;
&lt;code&gt;kubectl create -f replicaset-definition.yml&lt;/code&gt;: Creates the replicaset&lt;br&gt;
&lt;code&gt;kubectl get replicaset&lt;/code&gt;: Lists replicasets&lt;br&gt;
ReplicaSets can begin monitoring existing pods upon creation if said pods match the selector criteria&lt;br&gt;
&lt;code&gt;kubectl delete replicaset myapp-replicaset&lt;/code&gt;: Deletes the specified replicaset and the underlying pods\&lt;/p&gt;
&lt;h2 id=&#34;scale&#34;&gt;Scale&lt;/h2&gt;
&lt;p&gt;To scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manually edit the &lt;code&gt;replicas&lt;/code&gt; value in the definition file&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;kubectl scale --replicas=6 -f replicaset-definition.yml&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;kubectl scale --replicas=6 replicaset myapp-replicaset&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;deployments&#34;&gt;Deployments&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Rolling Updates&lt;/strong&gt;: Updating pods one by one to limit impact to accessibility&lt;br&gt;
&lt;strong&gt;Deployment&lt;/strong&gt;: Object that provides capability to upgrade with rolling updates, as well pause, and resume changes&lt;br&gt;
&lt;code&gt;deployment-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
	name: myapp-deployment
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-prod
			labels:
				app: myapp
				type: front-end
	spec:
		containers:
		- name: nginx-container
	      image: nginx
	replicas: 3
	selector:
		matchLabels:
			type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;      &lt;code&gt;kubectl create -f deployment-definition.yml&lt;/code&gt;: Creates the deployment&lt;br&gt;
      &lt;code&gt;kubectl get deployments&lt;/code&gt;: Lists deployments&lt;br&gt;
      &lt;code&gt;kubectl get all&lt;/code&gt;: Shows all objects created&lt;/p&gt;
&lt;h1 id=&#34;services-node-port&#34;&gt;Services: Node Port&lt;/h1&gt;
&lt;p&gt;Services enable connectivity between groups of pods and external sources&lt;br&gt;
Services are deployed as an object&lt;br&gt;
&lt;strong&gt;Node Port Service&lt;/strong&gt;: Listens to a port on the node and forward requests to a port running on the application&lt;br&gt;
      &lt;strong&gt;Target Port&lt;/strong&gt;: Port on the pod that requests are forwarded to&lt;br&gt;
      &lt;strong&gt;Port&lt;/strong&gt;: The port on the service&lt;br&gt;
      &lt;strong&gt;Node Port&lt;/strong&gt;: Physical port on the Node that ranges from 30008-3276&lt;br&gt;
&lt;strong&gt;Cluster IP&lt;/strong&gt;: Virtual IP inside the cluster to allow communication between different services&lt;br&gt;
&lt;strong&gt;LoadBalancer&lt;/strong&gt;: Distributes load based on set requirements&lt;br&gt;
&lt;code&gt;service-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
	name: myapp-service
	
spec:
	type: NodePort
	ports:
		- targetPort: 80
		port: 80
		nodePort: 30008
	selector:
		app: myapp
		type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;selector&lt;/code&gt;: Selects the applicable pods based on labels&lt;br&gt;
      The service will be applied to all pods with the label &lt;code&gt;app: myapp&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl get services&lt;/code&gt;: Lists services&lt;/p&gt;
&lt;h1 id=&#34;services-cluster-ip&#34;&gt;Services: Cluster IP&lt;/h1&gt;
&lt;p&gt;Pods are assigned non-static IP addresses when created&lt;br&gt;
A service can group like pods together and provide a single interface for access purposes&lt;br&gt;
&lt;code&gt;service-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
	name: back-end
	
spec:
	type: ClusterIP
	ports:
		- targetPort: 80
		port: 80
	selector:
		app: myapp
		type: back-end
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;services-loadbalancer&#34;&gt;Services: LoadBalancer&lt;/h1&gt;
&lt;p&gt;Kubernetes supports integration with cloud provider LoadBalancers&lt;br&gt;
Uses the same format as &lt;code&gt;NodePort&lt;/code&gt; YAML files, except type is set to &lt;code&gt;LoadBalancer&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
	name: myapp-service
	
spec:
	type: LoadBalancer
	ports:
		- targetPort: 80
		port: 80
		nodePort: 30008
	selector:
		app: myapp
		type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;namespaces&#34;&gt;Namespaces&lt;/h1&gt;
&lt;p&gt;Objects are created in the &lt;code&gt;default&lt;/code&gt; namespace that was made available during the creation of the cluster&lt;br&gt;
Another namespace called &lt;code&gt;kubesystem&lt;/code&gt; was also created for internal services&lt;br&gt;
A third namespace, &lt;code&gt;kubepublic&lt;/code&gt;, was created in which resources meant to made public to users are held&lt;br&gt;
Resources can be isolated in namespaces, i.e. Development vs. Production&lt;br&gt;
Resources within the same namespace can reach other by the object name:&lt;br&gt;
&lt;code&gt;mysql.connect(&amp;quot;db-service)&lt;/code&gt;&lt;br&gt;
To reach a service within another namespace, the name of the namespace needs to be appended to the service:&lt;br&gt;
&lt;code&gt;mysql.connect(&amp;quot;db-service.dev.svc.cluster.local&amp;quot;)&lt;/code&gt;&lt;br&gt;
      When the service was created, a DNS entry was created in this format:&lt;br&gt;
      &lt;code&gt;db-service&lt;/code&gt;: Sevice Name&lt;br&gt;
      &lt;code&gt;dev&lt;/code&gt;: Namespace&lt;br&gt;
      &lt;code&gt;svc&lt;/code&gt;: Service&lt;br&gt;
      &lt;code&gt;cluster.local&lt;/code&gt;: Domain&lt;br&gt;
&lt;code&gt;kubectl get pods -n [namespace]&lt;/code&gt;: List pods in a specified namespace&lt;br&gt;
&lt;code&gt;kubectle create -f definition.yml --namespace=[namespace]&lt;/code&gt;: Creates the object in the specified namespace&lt;br&gt;
The namespace can be applied in the &lt;code&gt;metadata&lt;/code&gt; section of a definition YAML file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
	name: myapp-prod
	namespace: dev
	labels:
		app: myapp
		type: front-end
spe
	containers:
		- name: nginx-container
		- image: nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To create a new namespace:&lt;br&gt;
&lt;code&gt;namespace-dev.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Namespace
metadata:
	name: dev
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A namespace can also be created with &lt;code&gt;kubectl create namespace dev&lt;/code&gt;&lt;br&gt;
To set the default namespace &lt;code&gt;kubectl&lt;/code&gt; reads from:\&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl config set-context $(kubectl config current-context) --namespace=namespace
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To limit resources in a namespace, create a Resource Quota:&lt;br&gt;
&lt;code&gt;compute-quota.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: ResourceQuota
metadata:
	name: compute-quota
	namespace: dev
spec:
	hard:
		pods: &amp;#34;10&amp;#34;
		requests.cpu: &amp;#34;4&amp;#34;
		requests.memory: 5Gi
		limits.cpu: &amp;#34;10&amp;#34;
		limits.memory: 10Gi
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;imperative-vs-declarative&#34;&gt;Imperative vs. Declarative&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Imperative&lt;/strong&gt;: A set of instructions written step by step&lt;br&gt;
      Using the &lt;code&gt;kubectl&lt;/code&gt; command for each step in order to deploy an object&lt;br&gt;
&lt;strong&gt;Declarative&lt;/strong&gt;: Requirements are declared and the system orchestrates the implementation process&lt;br&gt;
      Using a YAML file to deploy an object&lt;/p&gt;
&lt;h2 id=&#34;imperative&#34;&gt;Imperative&lt;/h2&gt;
&lt;h3 id=&#34;create-objects&#34;&gt;Create Objects&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl run --image=nginx nginx&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl create deployment --image=nginx nginx&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl expose deployment nginx --port 80&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl create deployment --image=nginx nginx --dry-run=client -o yaml&lt;/code&gt;&lt;br&gt;
      &lt;code&gt;--dry-run=client&lt;/code&gt;: Will not create the object and will just return with the status of the deployment&lt;br&gt;
      &lt;code&gt;-o yaml&lt;/code&gt;: Output the resource definition in YAML format&lt;br&gt;
&lt;code&gt;kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service&lt;/code&gt;: Create a Service named nginx of type NodePort to expose pod nginx&amp;rsquo;s port 80 on port 30080 on the nodes&lt;/p&gt;
&lt;h3 id=&#34;update-objects&#34;&gt;Update Objects&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl edit deployment nginx&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl scale deployment nginx --replicas=5&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl set image deployment nginx nginx=nginx:1.8&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl edit deployment nginx&lt;/code&gt;: Will edit the &lt;code&gt;pod-definition&lt;/code&gt; file in the Kubernetes Memory, which is separate from the YAML used to deploy the pod&lt;br&gt;
&lt;code&gt;kubectl replace -f nginx.yaml&lt;/code&gt;: Will replace the object with the object defined in the file&lt;br&gt;
      &lt;code&gt;--force&lt;/code&gt;: Will force the change&lt;/p&gt;
&lt;h2 id=&#34;declarative&#34;&gt;Declarative&lt;/h2&gt;
&lt;h3 id=&#34;create-objects-1&#34;&gt;Create Objects&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f nginx.yaml&lt;/code&gt;&lt;br&gt;
&lt;code&gt;kubectl apply -f /path/to/config-files&lt;/code&gt;: Will apply all of the files in the directory\&lt;/p&gt;
&lt;h3 id=&#34;update-objects-1&#34;&gt;Update Objects&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;kubectl apply -f nginx.yaml&lt;/code&gt;: Will update the object\&lt;/p&gt;
&lt;h1 id=&#34;kubectl-apply-command&#34;&gt;&lt;code&gt;kubectl apply&lt;/code&gt; Command&lt;/h1&gt;
&lt;p&gt;When the apply command is run, the object is created if it does not already exist&lt;br&gt;
A &lt;strong&gt;Live Object Configuration&lt;/strong&gt; file is created within the cluster, which includes extra values used for maintaining state&lt;br&gt;
The YAML is converted in JSON format and stored as the &lt;strong&gt;Last Applied Configuration&lt;/strong&gt;&lt;br&gt;
      This JSON is stored in the &lt;code&gt;annotations&lt;/code&gt; value in the Live Object Configuration file&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>/posts/welcome/</link>
      <pubDate>Mon, 23 Sep 2024 16:15:05 -0500</pubDate>
      
      <guid>/posts/welcome/</guid>
      <description>It Begins A journey of a thousand miles starts with a single step, and this site is mine. I’m Glenn Lewis, and I have been working in tech for 10 years. During that time, I’ve become adept at Linux administration, Python scripting, Incident Management, and System Observability. Like most people who haven’t gone through a typical education path, my knowledge had been built through hands-on experience and tribal knowledge.
What I would like to do with this site is track my continuing journey in software development and share my experience.</description>
      <content>&lt;h1 id=&#34;it-begins&#34;&gt;It Begins&lt;/h1&gt;
&lt;p&gt;A journey of a thousand miles starts with a single step, and this site is mine. I’m Glenn Lewis, and I have been working in tech for 10 years. During that time, I’ve become adept at Linux administration, Python scripting, Incident Management, and System Observability. Like most people who haven’t gone through a typical education path, my knowledge had been built through hands-on experience and tribal knowledge.&lt;/p&gt;
&lt;p&gt;What I would like to do with this site is track my continuing journey in software development and share my experience. I hope to share resources, pitfalls, and advice that others might find useful. Ellen Ulman once said “Programmers seem to be changing the world. It would be a relief, for them and for all of us, if they knew something about it,” and that is what I intend to do.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
