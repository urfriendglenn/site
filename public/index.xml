<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Terminal</title>
    <link>/</link>
    <description>Recent content on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 23 Sep 2024 19:44:48 -0500</lastBuildDate><atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Certified Kubernetes Administrator: Core Concepts</title>
      <link>/posts/cka_2/</link>
      <pubDate>Mon, 23 Sep 2024 19:44:48 -0500</pubDate>
      
      <guid>/posts/cka_2/</guid>
      <description>Cluster Architecture Control Plane Master Node: Manage, Plan, Schedule, and Monitor Nodes ETCD Cluster: Highly available key/value database kube-scheduler: Identifies the right node to place a container on Node-Controller: Manages nodes Replication-Controller: Ensures desired number of containers are running in a replication group kube-apiserver: Orchestrates all operations within the cluster and exposes the API
Workers Worker Nodes: Host applications as Containers A container runtime engine, like Docker, must be installed and available on all nodes kubelet: Agent that runs on each node in a cluster and listens for instructions from the API as well as monitors status of all the other nodes kube-proxy: Service ensures that the rules are in place on the worker nodes so the containers can communicate between each other</description>
      <content>&lt;h1 id=&#34;cluster-architecture&#34;&gt;Cluster Architecture&lt;/h1&gt;
&lt;h2 id=&#34;control-plane&#34;&gt;Control Plane&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Master Node&lt;/strong&gt;: Manage, Plan, Schedule, and Monitor Nodes
&lt;strong&gt;ETCD Cluster&lt;/strong&gt;: Highly available key/value database
&lt;strong&gt;kube-scheduler&lt;/strong&gt;: Identifies the right node to place a container on
&lt;strong&gt;Node-Controller&lt;/strong&gt;: Manages nodes
&lt;strong&gt;Replication-Controller&lt;/strong&gt;: Ensures desired number of containers are running in a replication group
&lt;strong&gt;kube-apiserver&lt;/strong&gt;: Orchestrates all operations within the cluster and exposes the API&lt;/p&gt;
&lt;h2 id=&#34;workers&#34;&gt;Workers&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Worker Nodes&lt;/strong&gt;: Host applications as Containers
A container runtime engine, like Docker, must be installed and available on all nodes
&lt;strong&gt;kubelet&lt;/strong&gt;: Agent that runs on each node in a cluster and listens for instructions from the API as well as monitors status of all the other nodes
&lt;strong&gt;kube-proxy&lt;/strong&gt;: Service ensures that the rules are in place on the worker nodes so the containers can communicate between each other&lt;/p&gt;
&lt;h1 id=&#34;docker-vs-containerd&#34;&gt;Docker vs. Containerd&lt;/h1&gt;
&lt;p&gt;Docker support was removed from Kubernetes, but Docker images still work because they adhere to imagespec
&lt;strong&gt;containerd&lt;/strong&gt;: Container runtime used by Docker that can be ran by itself
&lt;code&gt;ctr&lt;/code&gt;: Debugging tool bundled with containerd
&lt;code&gt;nerdctl&lt;/code&gt;: Provides a Docker-like for containerd
Supports docker compose
Supports newest features in containerd
&lt;code&gt;crictl&lt;/code&gt;: Provides a CLI for Container Runtime Interface (CRI) compatible container runtimes
Installed separately
Used to inspect and debug container runtimes
Works across different runtimes&lt;/p&gt;
&lt;h1 id=&#34;etcd-for-beginners&#34;&gt;ETCD for Beginners&lt;/h1&gt;
&lt;p&gt;ETCD is a distributed reliable key-value store
&lt;strong&gt;Key-Value Store&lt;/strong&gt;: Stores information as documents and all information on an item is contained in the corresponding document
To install ETCD, download and extract the binary then run the service
It will run on port 2379 and clients can be attached
&lt;code&gt;etcdctl&lt;/code&gt;: Command client
&lt;code&gt;etcdctl put key1 value1&lt;/code&gt;: Sets a key
&lt;code&gt;ectdctl get key1&lt;/code&gt;: Gets a key value&lt;/p&gt;
&lt;h1 id=&#34;etcd-in-kubernetes&#34;&gt;ETCD in Kubernetes&lt;/h1&gt;
&lt;p&gt;Stores information on the Nodes, PODs, Configs, Secrets, etc.
All changes are updated in the ETCD server, and is not considered completed until changed&lt;/p&gt;
&lt;h2 id=&#34;setup&#34;&gt;Setup&lt;/h2&gt;
&lt;h3 id=&#34;manual&#34;&gt;Manual&lt;/h3&gt;
&lt;p&gt;The binary is downloaded, then manually set as a service by executing the &lt;code&gt;etcd&lt;/code&gt; command with the required options&lt;/p&gt;
&lt;h2 id=&#34;kubeadm&#34;&gt;kubeadm&lt;/h2&gt;
&lt;p&gt;ETCD is deployed as a pod in the clusted
&lt;code&gt;kubectl exec etcd-master -n kube-system etcdctl get / --prefic -keys-only&lt;/code&gt;: Get keys stored in the ETCD pod&lt;/p&gt;
&lt;p&gt;In a HA Environment, there are multiple ETCD masters&lt;/p&gt;
&lt;h1 id=&#34;kube-api-server&#34;&gt;Kube-API Server&lt;/h1&gt;
&lt;p&gt;When running a command, it is sent to the kube-apiserver
POST requests can also be sent in place of commands:
&lt;code&gt;curl -X POST /api/v1/namespaces/default/pods ...&lt;/code&gt;: Creates a Pod
The scheduler detects there is a pod with no node assigned, then chooses a pod and communicates to the kube-apiserver
The apiserver instructs the kubelet to create a pod
The kubelet communicates the change to the apiserver, which updates ETCD&lt;/p&gt;
&lt;h1 id=&#34;kube-controller-manager&#34;&gt;Kube Controller Manager&lt;/h1&gt;
&lt;p&gt;A controller is a process that continuously monitors the system and takes necessary actions to keep the system in a specified state
&lt;strong&gt;Node Controller&lt;/strong&gt;: Responsible for monitoring the status of the nodes and takes action to keep applications running
Checks status every 5 second
After 40 seconds of no heartbeat, a node is marked unreachable
After 5 minutes, the node is evicted and pods are moved to healthy nodes
&lt;strong&gt;Replication Controller&lt;/strong&gt;: Responsible for monitoring the status of replicasets and ensures the number of pods are available within a set
If a pod dies, it creates another pod
There are many types of controllers in a Kubernetes Cluster, and are all packaged in the &lt;strong&gt;Kube-Controller-Manager&lt;/strong&gt;
kubeadm deploys the kube-controller-manager as a pod&lt;/p&gt;
&lt;h1 id=&#34;kube-scheduler&#34;&gt;Kube Scheduler&lt;/h1&gt;
&lt;p&gt;Responsible which pod goes on which node
Does not actually place the pod, the kubelet does
The scheduler looks at each pod and finds the correct node for it based on requirements (CPU, Memory, Labels, etc.)
The scheduler ranks the nodes and assigns on a rank from 0-10
kubeadm deploys the scheduler as a pod&lt;/p&gt;
&lt;h1 id=&#34;kubelet&#34;&gt;Kubelet&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Registers nodes&lt;/li&gt;
&lt;li&gt;Creates pods&lt;/li&gt;
&lt;li&gt;Monitors nodes and pods
The kubelet service is not installed by kubeadm and must be manually installed on each node&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;kube-proxy&#34;&gt;Kube Proxy&lt;/h1&gt;
&lt;p&gt;Every pod can reach every pod
A pod network is an internal network each pod communicates with
Applications are deployed as services so they can be available to the cluster
The service has an assigned IP
&lt;strong&gt;kube-proxy&lt;/strong&gt;: Process that runs on each that looks for new services, then creates the appropriate rules to route traffic through the pods
kubeadm deploys kube-proxy as a pod&lt;/p&gt;
&lt;h1 id=&#34;pods&#34;&gt;Pods&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Pod&lt;/strong&gt;: A single instance of an application and is the smallest object that can be created in Kubernetes
Containers are not added to existing pods to scale
Pods can container multiple containers, but the containers are not of the same image
&lt;code&gt;kubectl run nginx --image nginx&lt;/code&gt;: Deploys a docker container by creating a pod using the NGINX Docker Image
&lt;code&gt;kubectl get pods&lt;/code&gt;: Lists Pods&lt;/p&gt;
&lt;h1 id=&#34;pods-with-yaml&#34;&gt;Pods with YAML&lt;/h1&gt;
&lt;p&gt;Kubernetes uses YAML files as inputs for creation of objects
A Kubernetes definition file always contains the following four top-level fields:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
	name: myapp-prod
	labels:
		app: myapp
		type: front-end
spec:
	containers:
		- name: nginx-container
		- image: nginx
&lt;/code&gt;&lt;/pre&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;apiVersion&lt;/code&gt;: The version of the API the object will be created with&lt;/li&gt;
&lt;li&gt;&lt;code&gt;kind&lt;/code&gt;: Type of object being created&lt;/li&gt;
&lt;li&gt;&lt;code&gt;metadata&lt;/code&gt;: Data about the object in the form of a dictionary&lt;/li&gt;
&lt;li&gt;&lt;code&gt;spec&lt;/code&gt;: Specifications pertaining to the object
&lt;code&gt;kubectl create -f pod-definition.yml&lt;/code&gt;: Creates the object listed in the specified file
&lt;code&gt;kubectl describe pod myapp-pod&lt;/code&gt;: Lists information on the specified pod&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;replicasets&#34;&gt;ReplicaSets&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Replication Controller&lt;/strong&gt;: Runs multiple instances of a single Pod in a cluster
A replication controller also brings up new pods when a pod fails and ensures the necessary number of pods are running
Replication assists with load balancing and scaling by deploying multiple nodes in the cluster across nodes
&lt;strong&gt;ReplicaSet&lt;/strong&gt;: New recommended utility to manage replication
To create a replication controller:
&lt;code&gt;rc-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: ReplicationController
metadata:
	name: myapp-rc
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-prod
			labels:
				app: myapp
				type: front-end
	spec:
		containers:
		- name: nginx-container
	      image: nginx
	replicas: 3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;template&lt;/code&gt;: Pod template to be created by the replication controller, which will contain the contents of a Pod YAML file
&lt;code&gt;kubectl create -f rc-definition.yml&lt;/code&gt;: Creates the replication controller
&lt;code&gt;kubectl get replicationcontroller&lt;/code&gt;: Lists replication controller&lt;/p&gt;
&lt;p&gt;To create a replicaset:
&lt;code&gt;replicaset-definition.yml&lt;/code&gt;&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1
kind: ReplicaSet
metadata:
	name: myapp-replicaset
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-prod
			labels:
				app: myapp
				type: front-end
	spec:
		containers:
		- name: nginx-container
	      image: nginx
	replicas: 3
	selector:
		matchLabels:
			type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;selector&lt;/code&gt;: Identifies applicable pods, as ReplicaSets can manage pods that were not created by it. It is not a required field
&lt;code&gt;kubectl create -f replicaset-definition.yml&lt;/code&gt;: Creates the replicaset
&lt;code&gt;kubectl get replicaset&lt;/code&gt;: Lists replicasets
ReplicaSets can begin monitoring existing pods upon creation if said pods match the selector criteria
&lt;code&gt;kubectl delete replicaset myapp-replicaset&lt;/code&gt;: Deletes the specified replicaset and the underlying pods&lt;/p&gt;
&lt;h2 id=&#34;scale&#34;&gt;Scale&lt;/h2&gt;
&lt;p&gt;To scale:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manually edit the &lt;code&gt;replicas&lt;/code&gt; value in the definition file&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;kubectl scale --replicas=6 -f replicaset-definition.yml&lt;/code&gt; command&lt;/li&gt;
&lt;li&gt;Use the &lt;code&gt;kubectl scale --replicas=6 replicaset myapp-replicaset&lt;/code&gt; command&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;deployments&#34;&gt;Deployments&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Rolling Updates&lt;/strong&gt;: Updating pods one by one to limit impact to accessibility
&lt;strong&gt;Deployment&lt;/strong&gt;: Object that provides capability to upgrade with rolling updates, as well pause, and resume changes
&lt;code&gt;deployment-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: apps/v1
kind: Deployment
metadata:
	name: myapp-deployment
	labels:
		app: myapp
		type: front-end
spec:
	template:
		metadata:
			name: myapp-prod
			labels:
				app: myapp
				type: front-end
	spec:
		containers:
		- name: nginx-container
	      image: nginx
	replicas: 3
	selector:
		matchLabels:
			type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;kubectl create -f deployment-definition.yml&lt;/code&gt;: Creates the deployment
&lt;code&gt;kubectl get deployments&lt;/code&gt;: Lists deployments
&lt;code&gt;kubectl get all&lt;/code&gt;: Shows all objects created&lt;/p&gt;
&lt;h1 id=&#34;services-node-port&#34;&gt;Services: Node Port&lt;/h1&gt;
&lt;p&gt;Services enable connectivity between groups of pods and external sources
Services are deployed as an object
&lt;strong&gt;Node Port Service&lt;/strong&gt;: Listens to a port on the node and forward requests to a port running on the application
&lt;strong&gt;Target Port&lt;/strong&gt;: Port on the pod that requests are forwarded to
&lt;strong&gt;Port&lt;/strong&gt;: The port on the service
&lt;strong&gt;Node Port&lt;/strong&gt;: Physical port on the Node that ranges from 30008-32767
&lt;strong&gt;Cluster IP&lt;/strong&gt;: Virtual IP inside the cluster to allow communication between different services
&lt;strong&gt;LoadBalancer&lt;/strong&gt;: Distributes load based on set requirements
&lt;code&gt;service-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
	name: myapp-service
	
spec:
	type: NodePort
	ports:
		- targetPort: 80
		port: 80
		nodePort: 30008
	selector:
		app: myapp
		type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;code&gt;selector&lt;/code&gt;: Selects the applicable pods based on labels
The service will be applied to all pods with the label &lt;code&gt;app: myapp&lt;/code&gt;
&lt;code&gt;kubectl get services&lt;/code&gt;: Lists services&lt;/p&gt;
&lt;h1 id=&#34;services-cluster-ip&#34;&gt;Services: Cluster IP&lt;/h1&gt;
&lt;p&gt;Pods are assigned non-static IP addresses when created
A service can group like pods together and provide a single interface for access purposes
&lt;code&gt;service-definition.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
	name: back-end
	
spec:
	type: ClusterIP
	ports:
		- targetPort: 80
		port: 80
	selector:
		app: myapp
		type: back-end
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;services-loadbalancer&#34;&gt;Services: LoadBalancer&lt;/h1&gt;
&lt;p&gt;Kubernetes supports integration with cloud provider LoadBalancers
Uses the same format as &lt;code&gt;NodePort&lt;/code&gt; YAML files, except type is set to &lt;code&gt;LoadBalancer&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Service
metadata:
	name: myapp-service
	
spec:
	type: LoadBalancer
	ports:
		- targetPort: 80
		port: 80
		nodePort: 30008
	selector:
		app: myapp
		type: front-end
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;namespaces&#34;&gt;Namespaces&lt;/h1&gt;
&lt;p&gt;Objects are created in the &lt;code&gt;default&lt;/code&gt; namespace that was made available during the creation of the cluster
Another namespace called &lt;code&gt;kubesystem&lt;/code&gt; was also created for internal services
A third namespace, &lt;code&gt;kubepublic&lt;/code&gt;, was created in which resources meant to made public to users are held
Resources can be isolated in namespaces, i.e. Development vs. Production
Resources within the same namespace can reach other by the object name:
&lt;code&gt;mysql.connect(&amp;quot;db-service)&lt;/code&gt;
To reach a service within another namespace, the name of the namespace needs to be appended to the service:
&lt;code&gt;mysql.connect(&amp;quot;db-service.dev.svc.cluster.local&amp;quot;)&lt;/code&gt;
When the service was created, a DNS entry was created in this format:
&lt;code&gt;db-service&lt;/code&gt;: Sevice Name
&lt;code&gt;dev&lt;/code&gt;: Namespace
&lt;code&gt;svc&lt;/code&gt;: Service
&lt;code&gt;cluster.local&lt;/code&gt;: Domain
&lt;code&gt;kubectl get pods -n [namespace]&lt;/code&gt;: List pods in a specified namespace
&lt;code&gt;kubectle create -f definition.yml --namespace=[namespace]&lt;/code&gt;: Creates the object in the specified namespace
The namespace can be applied in the &lt;code&gt;metadata&lt;/code&gt; section of a definition YAML file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Pod
metadata:
	name: myapp-prod
	namespace: dev
	labels:
		app: myapp
		type: front-end
spe
	containers:
		- name: nginx-container
		- image: nginx
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To create a new namespace:
&lt;code&gt;namespace-dev.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: Namespace
metadata:
	name: dev
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A namespace can also be created with &lt;code&gt;kubectl create namespace dev&lt;/code&gt;
To set the default namespace &lt;code&gt;kubectl&lt;/code&gt; reads from:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl config set-context $(kubectl config current-context) --namespace=namespace
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To limit resources in a namespace, create a Resource Quota:
&lt;code&gt;compute-quota.yml&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;apiVersion: v1
kind: ResourceQuota
metadata:
	name: compute-quota
	namespace: dev
spec:
	hard:
		pods: &amp;#34;10&amp;#34;
		requests.cpu: &amp;#34;4&amp;#34;
		requests.memory: 5Gi
		limits.cpu: &amp;#34;10&amp;#34;
		limits.memory: 10Gi
&lt;/code&gt;&lt;/pre&gt;</content>
    </item>
    
    <item>
      <title>Resume</title>
      <link>/resume/</link>
      <pubDate>Mon, 23 Sep 2024 16:28:48 -0500</pubDate>
      
      <guid>/resume/</guid>
      <description>Glenn Lewis | DevOps Engineer 512-216-7591
Austin, TX 78748
glelewis90@gmail.com
LinkedIn | GitHub
Summary DevOps Engineer with ten years of technical experience in Linux systems administration, network management, and cloud architecture. Holds a certificate in Cybersecurity from the University of Texas’s Bootcamp. I am skilled in automation (SALT Ansible), scripting in Python and Bash, and corporate compliance. Has implemented ground-up infrastructure monitoring at two companies, implemented open-source host management solution via Proxmox, and has deployed and managed CI/CD via Jenkins and Kubernetes.</description>
      <content>&lt;h1 id=&#34;glenn-lewis--devops-engineer&#34;&gt;Glenn Lewis | DevOps Engineer&lt;/h1&gt;
&lt;p&gt;512-216-7591&lt;br&gt;
Austin, TX 78748&lt;br&gt;
&lt;a href=&#34;mailto:glelewis90@gmail.com&#34;&gt;glelewis90@gmail.com&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://www.linkedin.com/in/glenn-lewis-9363b458/&#34;&gt;LinkedIn&lt;/a&gt; | &lt;a href=&#34;https://github.com/urfriendglenn&#34;&gt;GitHub&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;DevOps Engineer with ten years of technical experience in Linux systems administration, network management, and cloud architecture. Holds a certificate in Cybersecurity from the University of Texas’s Bootcamp. I am skilled in automation (SALT Ansible), scripting in Python and Bash, and corporate compliance. Has implemented ground-up infrastructure monitoring at two companies, implemented open-source host management solution via Proxmox, and has deployed and managed CI/CD via Jenkins and Kubernetes. My strengths in project management and planning and my expertise in Linux administration make me an excellent addition to any technical team.&lt;/p&gt;
&lt;h1 id=&#34;skills&#34;&gt;Skills&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Front End&lt;/strong&gt;: HTML, CSS, JavaScript&lt;br&gt;
&lt;strong&gt;Backend&lt;/strong&gt;: Bash, MongoDB, MySQL, Salt, Ansible, Python, Django, Powershell, Git&lt;br&gt;
&lt;strong&gt;Cloud&lt;/strong&gt;: AWS, Google Cloud Platform&lt;br&gt;
&lt;strong&gt;On-Premise&lt;/strong&gt;: VMWare, IPMI, iDRAC, Networking&lt;br&gt;
&lt;strong&gt;Processes&lt;/strong&gt;: Incident Management, Agile, CI/CD, Project Management, GRC Compliance&lt;/p&gt;
&lt;h1 id=&#34;projects&#34;&gt;Projects&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/urfriendglenn/ElkStackDeployment&#34;&gt;&lt;strong&gt;Automated ELK Stack Deployment&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Summary&lt;/strong&gt;: In this project, Ansible was used to automate the deployment of docker containers that served three DVWA Hosts and an ELK stack.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Role&lt;/strong&gt;: Sole Author&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tools&lt;/strong&gt;: Azure, Ansible, ELK, Beats&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;education&#34;&gt;Education&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;University of Texas, Austin, TX&lt;/strong&gt; - &lt;em&gt;Cybersecurity Bootcamp Certificate&lt;/em&gt; (June 2021-Dec 2021)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Morton Ranch High School, Katy, TX&lt;/strong&gt; - &lt;em&gt;High School Diploma&lt;/em&gt; (2004-2008)&lt;/p&gt;
&lt;h1 id=&#34;experience&#34;&gt;Experience&lt;/h1&gt;
&lt;h2 id=&#34;vectra-ai--austin-tx---_devops-engineer-ii_&#34;&gt;Vectra AI | Austin, TX - &lt;em&gt;DevOps Engineer II&lt;/em&gt;&lt;/h2&gt;
&lt;h3 id=&#34;june-2021---present&#34;&gt;June 2021 - Present&lt;/h3&gt;
&lt;p&gt;Responsibilities included deploying and maintaining a monitoring system, Zabbix, for the internal corporate infrastructure, which comprises approximately 85% of Vectra’s entire infrastructure.&lt;/p&gt;
&lt;h4 id=&#34;key-accomplishments&#34;&gt;Key Accomplishments&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Deployed monitoring and eventing for internal infrastructure via Zabbix, ELK, Grafana, and PagerDuty&lt;/li&gt;
&lt;li&gt;Designed, planned, and deployed Linux systems based on given requirements to both on-premise
architecture, via VMWare, and cloud, via AWS&lt;/li&gt;
&lt;li&gt;Deployed and maintained systems through automation via SALT and Ansible&lt;/li&gt;
&lt;li&gt;Deployed, configured, and maintained self-hosted virtualization via VMWare (ESXi, vSphere, and
vCenter)&lt;/li&gt;
&lt;li&gt;Spearheaded and implemented System Architecture version continuity program&lt;/li&gt;
&lt;li&gt;Executed GRC and Vulnerability related tasks&lt;/li&gt;
&lt;li&gt;Managed CI/CD Pipeline via Jenkins, Docker, and Kubernetes&lt;/li&gt;
&lt;li&gt;Designed and deployed microservices via Docker, Podman, and Kubernetes&lt;/li&gt;
&lt;li&gt;Managed SQL and MongoDB databases&lt;/li&gt;
&lt;li&gt;Developed and contributed code written in Python&lt;/li&gt;
&lt;li&gt;Managed data pipeline via ELK and Grafana&lt;/li&gt;
&lt;li&gt;Deployed, Configured, and Maintained On-Premise Host Management Solution via Proxmox&lt;/li&gt;
&lt;li&gt;Deployed, Configured, and Maintained Kubernetes Clusters via K3S&lt;/li&gt;
&lt;li&gt;Designed and Deployed Automated Scans via Tenable, Jenkins, and Kubernetes&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;procore-technologies--austin-tx---_senior-information-systems-engineer_&#34;&gt;Procore Technologies | Austin, TX - &lt;em&gt;Senior Information Systems Engineer&lt;/em&gt;&lt;/h2&gt;
&lt;h3 id=&#34;aug-2019---june-2021&#34;&gt;Aug 2019 - June 2021&lt;/h3&gt;
&lt;p&gt;As a Senior Information Systems Engineer, my responsibilities included designing and implementing monitoring for Procore’s corporate internal infrastructure. Responsibilities included creating the initial runbooks and standard operating procedures for an integrated operations center.&lt;/p&gt;
&lt;h4 id=&#34;key-accomplishments-1&#34;&gt;Key Accomplishments&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Promotion from Information Systems Engineer to Senior Information Systems Engineer&lt;/li&gt;
&lt;li&gt;Lead the procurement and implementation of LogicMonitor&lt;/li&gt;
&lt;li&gt;Commanded and assisted in over 100 app and security incidents following ITIL standards&lt;/li&gt;
&lt;li&gt;Led and designed implementation of monitoring and response for Procore’s authorized ecosystem&lt;/li&gt;
&lt;li&gt;Assisted in the endeavor of the company going public by implementing monitoring for critical
components and systems&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;q2ebanking--austin-tx---_noc-analyst-ii_&#34;&gt;Q2ebanking | Austin, TX - &lt;em&gt;NOC Analyst II&lt;/em&gt;&lt;/h2&gt;
&lt;h3 id=&#34;jan-2019---aug-2019&#34;&gt;Jan 2019 - Aug 2019&lt;/h3&gt;
&lt;p&gt;At Q2ebanking, responsibilities included system support activities, such as network and server monitoring, troubleshooting, escalation, and resolution. Responsibilities also included customer outreach, system upgrades, and initial security response.&lt;/p&gt;
&lt;h4 id=&#34;key-accomplishments-2&#34;&gt;Key Accomplishments&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ensured that SLA for event response of 98% was met&lt;/li&gt;
&lt;li&gt;Worked on rotating shifts between day and night&lt;/li&gt;
&lt;li&gt;Direct contact with customer escalations&lt;/li&gt;
&lt;/ul&gt;
</content>
    </item>
    
    <item>
      <title>Welcome</title>
      <link>/posts/welcome/</link>
      <pubDate>Mon, 23 Sep 2024 16:15:05 -0500</pubDate>
      
      <guid>/posts/welcome/</guid>
      <description>It Begins A journey of a thousand miles starts with a single step, and this site is mine. I’m Glenn Lewis, and I have been working in tech for 10 years. During that time, I’ve become adept at Linux administration, Python scripting, Incident Management, and System Observability. Like most people who haven’t gone through a typical education path, my knowledge had been built through hands-on experience and tribal knowledge.
What I would like to do with this site is track my continuing journey in software development and share my experience.</description>
      <content>&lt;h1 id=&#34;it-begins&#34;&gt;It Begins&lt;/h1&gt;
&lt;p&gt;A journey of a thousand miles starts with a single step, and this site is mine. I’m Glenn Lewis, and I have been working in tech for 10 years. During that time, I’ve become adept at Linux administration, Python scripting, Incident Management, and System Observability. Like most people who haven’t gone through a typical education path, my knowledge had been built through hands-on experience and tribal knowledge.&lt;/p&gt;
&lt;p&gt;What I would like to do with this site is track my continuing journey in software development and share my experience. I hope to share resources, pitfalls, and advice that others might find useful. Ellen Ulman once said “Programmers seem to be changing the world. It would be a relief, for them and for all of us, if they knew something about it,” and that is what I intend to do.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 23 Sep 2024 00:00:00 +0000</pubDate>
      
      <guid>/about/</guid>
      <description>I am a DevOps Engineer with ten years of technical experience in Linux systems administration, network management, and cloud architecture. I hold a certificate in Cybersecurity from the University of Texas’s Bootcamp. I am skilled in automation (SALT, Ansible), scripting in Python and Bash, and corporate compliance. Has implemented ground-up infrastructure monitoring at two companies, implemented open-source host management solution via Proxmox, and has deployed and managed CI/CD via Jenkins and Kubernetes.</description>
      <content>&lt;p&gt;I am a DevOps Engineer with ten years of technical experience in Linux systems administration, network management, and cloud architecture. I hold a certificate in Cybersecurity from the University of Texas’s Bootcamp. I am skilled in automation (SALT, Ansible), scripting in Python and Bash, and corporate compliance. Has implemented ground-up infrastructure monitoring at two companies, implemented open-source host management solution via Proxmox, and has deployed and managed CI/CD via Jenkins and Kubernetes. My strengths in project management and planning and my expertise in Linux administration make me an excellent addition to any technical team.&lt;/p&gt;
&lt;p&gt;I am a writer, reader, guitarist, and runner. I enjoy traveling with my wife and dog.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
